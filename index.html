<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="VLM, Math AIF, Smart Vision-Language Reasoners, Smarter-VLM">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Smart Vision-Language Reasoners</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PMFB4D98V6"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PMFB4D98V6');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/orgs/smarter-vlm/repositories">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://scholar.google.com/citations?user=FN8FNYoAAAAJ&hl=en">
           D Roberts research
          </a>
          <a class="navbar-item" href="https://scholar.google.com/citations?user=_kr2jm8AAAAJ&hl=en">
            L Roberts research
          </a>
	  <!-- etc...
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
	  -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Smart Vision-Language Reasoners</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.denisaroberts.me/">Denisa Roberts</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://rlucas7.github.io/">Lucas Roberts</a><sup>2</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>New York University</span>
            <span class="author-block"><sup>2</sup>Yext Inc</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <!-- <span class="link-block">
                <a href="./static/paper/smartVLM_ai4math_icml24.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span> -->
                <!-- </a>
              </span> -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2407.04212"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
	      <!-- tODO: make a brief video
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
	      -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/smarter-vlm/smarter"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://zenodo.org/records/7775984"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
	    <img src="./static/images/reason_and_qf.png" class="" id="teaser" alt="Smarter-VLM Architecture for learning and reasoning model"/>
	    <!--
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
	    -->
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Smarter-VLM</span>, Smart Vision-Language Reasoners improve performance on Math problems by up to 48%.
      </h2>
    </div>
  </div>

</section>


<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
            <img src="./static/images/puzzle1.png" class="" id="fullbody" alt="Puzzle 3"/>
        </div>
        <div class="item item-chair-tp">
          <img src="./static/images/puzzle2.png" class="" id="chair-tp" alt="example SMART data problem."/>
        </div>
        <div class="item item-shiba">
          <img src="./static/images/puzzle3.png" class="" id="shiba" alt="example SMART data problem."/>
        </div>
        <div class="item item-fullbody">
          <img src="./static/images/puzzle4.png" class="" id="fullbody" alt="Puzzle 3"/>
	</div>
        <div class="item item-blueshirt">
          <img src="./static/images/puzzle5.png" class="" id="blueshirt" alt="Puzzle 4"/>
        </div>
        <div class="item item-mask">
          <img src="./static/images/puzzle6.png" class="" id="mask" alt="Puzzle 5"/>
        </div>
        <div class="item item-coffee">
          <img src="./static/images/puzzle7.png" class="" id="coffee" alt="Puzzle 6"/>
        </div>
        <div class="item item-toby">
          <img src="./static/images/puzzle8.png" class="" id="toby" alt="Puzzle 7"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
	  We investigate vision-language models (VLM) as reasoners.
	  The ability to form abstractions underlies mathematical reasoning, problem-solving, and other Math AI tasks. Several formalisms have been given to these underlying abstractions and skills utilized by humans and intelligent systems for reasoning.
          </p>Furthermore, human reasoning is inherently multimodal, and as such, we focus our investigations on multimodal AI. In this article, we employ the abstractions given in the SMART task (Simple Multimodal Algorithmic Reasoning Task) introduced in
	  <a href="https://arxiv.org/abs/2212.09993">Cherian 2022</a>
	  as meta-reasoning and problem-solving skills along eight axes: math, counting, path, measure, logic, spatial, and pattern.
          <p>
          </p>We investigate the ability of vision-language models to reason along these axes and seek avenues of improvement. Including composite representations with vision-language cross-attention enabled learning multimodal representations adaptively from fused frozen pretrained backbones for better visual grounding. Furthermore, proper hyperparameter and other training choices led to strong improvements (up to 48% gain in accuracy) on the SMART task, further underscoring the power of deep multimodal learning. The smartest VLM, which includes a novel QF multimodal layer, improves upon the best previous baselines in every one of the eight fundamental reasoning skills.
          <p>
          </p>
        </div>
      </div>
    </div>

    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe-->
        <!-- </div> -->
      <!-- </div> -->
    <!-- </div> --> 
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">

      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <h2 class="title is-3">Visualizations and Ablations</h2>
          <p>
	  We release full ablation experiment results and training curves tracked in CometML.
          </p>
	  <p> A
	      <a href="https://www.comet.com/droberts308/multimodalai/view/QF0ah3akqYB6IiNuyVXuRchlh/panels">Comet Dashboard</a>
	      preview is shown here in the panel.
	  </p>
        </div>
      </div>
      </div>
    </div>

<section class="section">
  <div class="container is-max-desktop">
     <div class="publication-video">
	  <iframe src="https://www.comet.com/droberts308/multimodalai/view/QF0ah3akqYB6IiNuyVXuRchlh/panels"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
	  <div class="publication-video">
       </div>
   </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!--div class="column"-->
        <!--
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
	-->
      <!--/div-->
    <!--/div-->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!--div class="columns is-centered"-->
      <!--div class="column is-full-width"-->
	<!--
        <h2 class="title is-3">Animation</h2>
	-->
        <!-- Interpolating. -->
	<!--
        <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/>
	-->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
	<!--
        <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div>
	-->
        <!--/ Re-rendering. -->

      <!--/div-->
    <!--/div-->
    <!--/ Animation. -->


    <!-- Concurrent Work. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
	  Many related excellent works have been introduced recently.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2212.09993">
		    Are Deep Neural Networks SMARTer than Second Graders?
	    </a>
	    introduced the Simple Multi-modal Algorithmic Reasoning Task
	    <a href="https://smartdataset.github.io/smart101"/>(SMART)</a> 
      and the SMART-101 dataset and trained a first set of baseline
	    vision-language reasoners to solve this task.
          </p>
	  <p> <a href="https://arxiv.org/abs/2310.10631">
		  Llemma: An Open Language Model For Mathematics
	  </a>
	  presented in the Math AI Workshop at NeurIPS 2023, trains a large
	  language model on Proof-Pile-2 dataset as well as web data and math
	  code (Lean) to train a large language model which can reason
	  mathematically.
	  </p>
	  <p>
	  <a href="https://arxiv.org/abs/2401.06209">
		  Eyes Wide Shut? Exploring the Visual Shortcomings of
		  Multimodal LLMs,
	  </a>
	  an article in CVPR 2024, reveals that visual capabilities in large
	  multimodal models are systematically lagging behind the powerful
	  reasoning abilities of large language models and motivates our
	  inclusion of several visual representations for better multimodal
	  reasoning.
	  </p>
	  <p>
	  <a href="https://arxiv.org/abs/2403.14624">
		 Mathverse: Does your multi-modal LLM truly see the diagrams in
		 visual math problems?
	  </a>
	  motivates our exploration of deep learning architectures for better
	  visual grounding since large multimodal models cannot truly
	  understand the visual diagrams for mathematical reasoning.
	  </p>
          <p>
            There are probably many more by the time you are reading this.
          </p>
        </div>
      </div>
    </div>
    <!--/ Concurrent Work. -->

  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <h2 class="title is-3">Math AI ICML 2024 workshop poster</h2>
	    <embed src="./static/images/poster_icml_2024_workshop.pdf" width="800px" height="1200px"/>
	    <!--img src="./static/images/poster_icml_2024_workshop.pdf" class="" id="teaser" alt="ICML 2024 Math AI workshop poster"/-->
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{roberts2024smartvisionlanguagereasoners,
  author        = {Denisa Roberts and Lucas Roberts},
  title         = {Smart Vision-Language Reasoners},
  year          = {2024},
  eprint	= {2407.04212},
  archivePrefix = {arXiv},
  primaryClass  = {cs.AI},
  url		= {https://arxiv.org/abs/2407.04212},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/abs/2407.04212">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/smarter-vlm/" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            and with great acknowledgements to nerfies project: <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a>, as the starter code of this website,
          </p>
	  <p>
	  This research effort was conducted independently of Yext Employment.
	  No yext resources nor work hours were used to conduct the research.
	  </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
